---
title: "Micrograd from Scratch"
format: html
---

## Setup Code

```{python}
import math
import numpy as np
import matplotlib.pyplot as plt
from graphviz import Digraph
```

```{python}
print(2+2)
```

## Micrograd

Github library `karpathy/micrograd`

- Autograd engine
- Implements back-propagation
- Efficiently evaluate the gradient of a loss function wrt the weights of a neural network
- Tune the weights of a neural network efficiently
- Builds the graph automatically
    - Forward and backward pass
- Backward pass: chain rule to calculate the derivatives
    - How values effect others through the gradient

Neural networks

- Just mathematical expression
- Data + weights $\rightarrow$ Outputs
- Backprop is not specific to neural networks

Micrograd uses scalars, but torch will use tensors.

- Math does not change on tensors
- Tensors are arrays of tensors
- Parallelized
- Everything else is efficiency

## Derivatives

```{python}
def f(x):
    return 3 * x ** 2 - 4 * x + 5
```

```{python}
f(3.0)

xs = np.arange(-5, 5.25, 0.25)
ys = f(xs)

plt.plot(xs, ys)
```

Derivatives without derivatives.

- NN doesn't do this symbolically.
- What is the sensitivity of a function at a certain value of $x$

Numerical approximation of the slope at $x$:

```{python}
h = 0.001
x = 3.0

(f(x + h) - f(x)) / h
```

- Rise: $(f(x + h) - f(x))$
- Run: $h$

Negative slopes at $x = -3$:

```{python}
h = 0.001
x = -3.0

(f(x + h) - f(x)) / h
```

Zero slope at $x = 2/3$

```{python}
h = 0.001
x = 2.0 / 3.0

(f(x + h) - f(x)) / h
```

## Function with three scalar inputs

```{python}
a = 2.0
b = -3.0
c = 10.0
d = a * b + c
print(d)
```

Derivative of $d$ as a function of $a$, $b$, and $c$

```{python}
h = 0.001

# inputs
a = 2.0
b = -3.0
c = 10

d1 = a * b + c
a += h
d2 = a * b + c

print('d1', d1)
print('d2', d2)
print('slope', (d2 - d1) / h)

d1 = a * b + c
b += h
d2 = a * b + c

print('d1', d1)
print('d2', d2)
print('slope', (d2 - d1) / h)

d1 = a * b + c
c += h
d2 = a * b + c

print('d1', d1)
print('d2', d2)
print('slope', (d2 - d1) / h)
```

## Data structures to hold values

```{python}
class Value:
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label

    def __repr__(self):
        return f"Value(data={self.data})"
    
    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), '+')

        def _backward():
            self.grad += 1.0 * out.grad
            other.grad += 1.0 * out.grad
        
        out._backward = _backward

        return out

    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), '*')

        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad

        out._backward = _backward

        return out

    def tanh(self):
        x = self.data
        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)
        out = Value(t, (self, ), 'tanh')

        def _backward():
            self.grad += (1 - t ** 2) * out.grad

        out._backward = _backward
        return out

    def backward(self):
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)

        self.grad = 1.0
        for node in reversed(topo):
            node._backward()


a = Value(2.0, label = 'a')
b = Value(-3.0, label = 'b')
print(a + b)

c = Value(10.0, label = 'c')
print(a * b)
```

## Previous, children, operation

We now know how each value was created

?? Should be -6 and 10, not 10 and -6 ??

```{python}
e = a * b
e.label = 'e'
d = e + c
d.label = 'd'
f = Value(-2.0, label = 'f')
L = d * f
L.label = 'L'
print(L)
```

## Visualization of expression graphs

```{python}
def trace(root):
    nodes, edges = set(), set()
    
    # Build a set of nodes and edges
    def build(v):
        if v not in nodes:
            nodes.add(v)
            for child in v._prev:
                edges.add((child, v))
                build(child)
    build(root)
    return nodes, edges
      
def draw_dot(root):
    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # L to R
    nodes, edges = trace(root)

    for n in nodes:
        uid = str(id(n))

        # for any value in the graph create a rectangular record
        dot.node(name = uid,
                 label = "{ %s | data %.4f | grad %0.4f}" % (n.label, n.data, n.grad), 
                 shape='record')
        if n._op:
            # if the value is a result of an operation, create an op node
            dot.node(name = uid + n._op, label = n._op)

            # and connect this node to it
            dot.edge(uid + n._op, uid)
    for n1, n2, in edges:
        # connect n1 to the op node of n2
        dot.edge(str(id(n1)), str(id(n2)) + n2._op)
    
    return dot

draw_dot(L)
```

Output of the forward pass is `{python} print(L.data)`.

## Back-propagation

Start at $L$ and calculate the gradient in reverse.

- Derivative of each node wrt $L$
- Data is fixed, so we don't calculate the derivatives for data
- Recursively multiply the derivatives along the path

$$
\frac{dL}{dL} = 1
$$

Chain rule

$$
\frac{dz}{dx} = \frac{dz}{dy} \cdot \frac{dy}{dx}
$$

 Gradient gives you the power to influence the output.

 ## Back-propagation through a neuron

tanh activation function

- Inputs are squashed to the range -1 and 1
- Useful in the range -2 to 2


```{python}
plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)))
plt.grid()
```

Weights


```{python}
# inputs x1 and x2
x1 = Value(2.0, label = 'x1')
x2 = Value(0.0, label = 'x2')

# weights w1 and w2
w1 = Value(-3.0, label = 'w1')
w2 = Value(1.0, label = 'w2')

# b is the bias of the neuron
b = Value(6.8813735870195432, label = 'b')

x1w1 = x1 * w1; x1w1.label = 'x1 * w1'
x2w2 = x2 * w2; x2w2.label = 'x2 * w2'

x1w1x2w1 = x1w1 + x2w2; x1w1x2w1.label = '(x1 * w1) + (x2 * w2)'

# n is the cell body activation without the activation function
n = x1w1x2w1 + b; n.label = 'n'

draw_dot(n)

# Implement tanh above
o = n.tanh(); o.label = 'o'
draw_dot(o)
```

Derivatives on the weights is the important part

Back-propagation manually from $o$:

```{python}
# o.grad = 1.0
# n.grad = 0.5
# x1w1x2w1.grad = 0.5
# b.grad = 0.5
# x1w1.grad = 0.5
# x2w2.grad = 0.5
# x2.grad = w2.data * x2w2.grad
# w2.grad = x2.data * x2w2.grad
# x1.grad = w1.data * x1w1.grad
# w1.grad = x1.data * x1w1.grad

# # o = tanh(n)
# # ? do / dn = 1 - tanh(n)**2

# 1 - o.data ** 2

# draw_dot(o)
```

## Backward pass automatically

Implement `backward` above

```{python}
o.grad = 1.0
o._backward()
n._backward()
b._backward()
x1w1x2w1._backward()
x1w1._backward()
x2w2._backward()
draw_dot(o)
```

We don't want to have to set `o.grad` manually. Use topological sort of the DAG left to right.

```{python}
o.backward()
draw_dot(o)
```

Bug: Gradient is incorrect (should be 2). Gradient is overwritten because self and other point to the same memory location.

- Need to accumulate the derivatives `+=`.


```{python}
a = Value(3.0, label = 'a')
b = a + a; b.label = 'b'
b.backward()
draw_dot(b)
```