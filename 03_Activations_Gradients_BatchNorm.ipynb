{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations, Gradients, BatchNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formerly:\n",
    "\n",
    "- Used MLP to predict the next character in the sequence\n",
    "\n",
    "Now:\n",
    "\n",
    "- A little more about MLPs: activations and gradients\n",
    "- \"Universal approximator\" but not easily optimizable because of activations and gradients\n",
    "- Newer approaches attempt to handle activations and gradients better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in all the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the vocabulary of characters and mappings to/from integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):\n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up general MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 10    # dimensionality of the character embedding vectors\n",
    "n_hidden = 200  # number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embed), generator = g)\n",
    "W1 = torch.randn((n_embed * block_size, n_hidden), generator = g) * (5/3) / ((n_embed * block_size)**0.5)\n",
    "# b1 = torch.randn(n_hidden, generator = g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.01 # Scale down but not 0\n",
    "b2 = torch.randn(vocab_size, generator = g) * 0 # Initialize at 0\n",
    "\n",
    "# Scale and shift\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(f'Network size: {sum(p.nelement() for p in parameters)}')\n",
    "\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "loss_ii = []\n",
    "\n",
    "for ii in range(max_steps):\n",
    "  # Minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator = g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix]  # Batches X and Y\n",
    "\n",
    "  # Forward pass\n",
    "  emb = C[Xb] # Embed characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # Concatenate the vectors\n",
    "\n",
    "  # Linear layer\n",
    "  # Note no b1 here, because of standardization below. All these\n",
    "  # gradients are 0. bnbias takes care of it.\n",
    "  hpreact = embcat @ W1 # Hidden layer preactivation\n",
    "\n",
    "  # Batch normalization after linear layer\n",
    "  # Add a small amount to std_ii to avoid divide by zero\n",
    "  bnmean_ii = hpreact.mean(0, keepdim = True)\n",
    "  bnstd_ii = hpreact.std(0, keepdim = True)\n",
    "  hpreact = bngain * (hpreact - bnmean_ii) / (bnstd_ii + 1e-5) + bnbias\n",
    "\n",
    "  # Update running mean and sd (don't keep gradients)\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmean_ii\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstd_ii\n",
    "\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "  # Backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None # initialize to 0\n",
    "  loss.backward()\n",
    "\n",
    "  # Update\n",
    "  lr = 0.1 if ii < 100000 else 0.01\n",
    "  for p in parameters:\n",
    "      p.data += -lr * p.grad\n",
    "  \n",
    "  # Print\n",
    "  if ii % 10000 == 0:\n",
    "    print(f'{ii:7d} / {max_steps:7d}: {loss.item():.4f}')\n",
    "\n",
    "  # Keep track of stats\n",
    "  loss_ii.append(loss.log10().item())\n",
    "\n",
    "  # if ii > 1000:\n",
    "  #   break\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization is not very good (27 is really high)\n",
    "\n",
    "- The distribution should be uniform at the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-torch.tensor(1 / 27.0).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should be ~3.3\n",
    "\n",
    "Scale down W2 and b2 at initialization.\n",
    "\n",
    "- Set b2 to 0\n",
    "- Scale W2 to small numbers to tame the loss at initialization\n",
    "- Spend more time optimizing the NN\n",
    "\n",
    "At initialization, `tanh` is pushing too many values to -1 and 1:\n",
    "\n",
    "- Backpropagation through `tanh` with -1 or 1 stops the backpropagation.\n",
    "- Changing the input doesn't change the output (no impact on loss)\n",
    "- Gradient is squashed\n",
    "- When `tanh` = 0, `out.grad` is passed through\n",
    "- `hpreact` is too broad (-15 to 15) at initialization\n",
    "- Scale down `b1` and `W1`\n",
    "\n",
    "\"Dead neurons\" happen when all the gradients are zeroed for a single neuron.\n",
    "\n",
    "- Can happen for tanh, relu, sigmoid or any distribution with a flat region\n",
    "- Other activation functions have been developed to avoid this flatness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(h.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hpreact.view(-1).tolist(), 50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White are saturated neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 15))\n",
    "plt.imshow(h.abs() > 0.99, cmap = 'gray', interpolation = 'nearest');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(loss_ii);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare train and validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # Decorator to disable gradient tracking in the following\n",
    "def split_loss(split):\n",
    "  x, y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte)\n",
    "  }[split]\n",
    "\n",
    "  # Forward pass\n",
    "  emb = C[x]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # Hidden layer preactivation\n",
    "\n",
    "  # Can't calculate the local batch normalization for single samples, so\n",
    "  # use the running values from above.\n",
    "  # hpreact = bngain * (hpreact - hpreact.mean(0, keepdim = True)) / hpreact.std(0, keepdim = True) + bnbias\n",
    "\n",
    "  # Use the calculated batch normalization from above\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "\n",
    "  h = torch.tanh(hpreact)\n",
    "  logits = h @ W2 + b2\n",
    "\n",
    "  # Evaluate loss\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "  out = []\n",
    "  context = [0] * block_size\n",
    "\n",
    "  while True:\n",
    "    emb = C[torch.tensor([context])]\n",
    "    h = torch.tanh(emb.view(1, -1) @ W1)\n",
    "    logits = h @ W2 + b2\n",
    "    probs = F.softmax(logits, dim = 1)\n",
    "\n",
    "    # Sample from the multinomial distribution\n",
    "    ix = torch.multinomial(probs, num_samples = 1, generator = g).item()\n",
    "\n",
    "    # Shift the context window and track the samples\n",
    "    context = context[1:] + [ix]\n",
    "    out.append(ix)\n",
    "\n",
    "    # Break if we hit '.'\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(itos[i] for i in out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is shallow and simple, so the optimization eventually works fine, even with the poor initialization. This will not be the case with much deeper networks (e.g., 50 layers).\n",
    "\n",
    "- Bad initialization can mean that the network will not trait at all.\n",
    "\n",
    "How do you determine what the values are for scaling the initialization (0.2, 0.01, etc. above)?\n",
    "\n",
    "Consider matrix multiplication of random normal values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200)\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "\n",
    "plt.figure(figsize = (20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1), 50, density = True);\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1), 50, density = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean stays at 0, sd goes from 1 to 3.\n",
    "\n",
    "How can we preserve the standard normal?\n",
    "\n",
    "- Divide by sqrt of \"fan_in\" (here, 10)\n",
    "- See He et al., 2015 about CNN and ReLU / PReLu\n",
    "- If the forward pass is optimized, the backward pass will be as well\n",
    "- See `torch.nn.kaiming_normal` for standard initialization built-in to pytorch\n",
    "    - Has proper gain for the activation function\n",
    "- Modern innovations have made the initialization less crucial: better optimizers, other activations, batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 10)\n",
    "w = torch.randn(10, 200) / 10**0.5\n",
    "y = x @ w\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n",
    "\n",
    "plt.figure(figsize = (20, 5))\n",
    "plt.subplot(121)\n",
    "plt.hist(x.view(-1), 50, density = True);\n",
    "plt.subplot(122)\n",
    "plt.hist(y.view(-1), 50, density = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization\n",
    "\n",
    "- Don't preactivation states to be too small or too large.\n",
    "- Just standardize the hidden states to make them standard Normal\n",
    "- Work on `hpreact`\n",
    "- Only want it to be standardized at initialization. Want it to be able to move around in backpropagation.\n",
    "    - Scale and shift as a final step.\n",
    "\n",
    "We still don't get much improvement from batch normalization because the network is pretty small.\n",
    "\n",
    "- Scatter batch normalization around the network. One per linear or convolutional layer\n",
    "- There is a cost: neurons are not independent anymore, depending on what else is in the batch will change `h` and `logits`.\n",
    "- This acts as a regularizer via (a kind of) data augmentation\n",
    "\n",
    "Batch normalization causes difficulties for new observation prediction, because it expects to calculate mean and standard deviation for single observations.\n",
    "\n",
    "- One solution is to have a post-training step that fixes the batch normalization one time.\n",
    "- Better solution is to do the updating during training, updating a little at each iteration (implemented above)\n",
    "\n",
    "Group and layer normalization are newer versions of batch normalization.\n",
    "\n",
    "## Implementation in torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "  def __init__(self, fan_in, fan_out, bias = True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "\n",
    "  def __init__(self, dim, eps = 1e-5, momentum = 0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "\n",
    "    # Parameters trained (with backpropagation)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "    # Buffers (trained with running momentum update)\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "    \n",
    "  def __call__(self, x):\n",
    "\n",
    "    # Calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdims = True) # Batch mean\n",
    "      xvar = x.var(0, keepdims = True, unbiased = True) # Batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "    # Update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator = g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "  Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # Last layer: make less confident\n",
    "  # layers[-1].weight += 0.1\n",
    "  layers[-1].gamma += 0.1\n",
    "\n",
    "\n",
    "  # All other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "print(f'Number of parameters: {sum(p.nelement() for p in parameters)}')\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "loss_ii = []\n",
    "ud = []\n",
    "\n",
    "for ii in range(max_steps):\n",
    "\n",
    "  # Minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator = g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "  # Forward pass\n",
    "  emb = C[Xb] # Embed characters into a vector\n",
    "  x = emb.view(emb.shape[0], -1) # Concatenate the vectors\n",
    "  for layer in layers:\n",
    "      x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "  # Backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad()\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "\n",
    "  # Update\n",
    "  lr = 0.1 if ii < 10000 else 0.01\n",
    "  for p in parameters:\n",
    "      p.data += -lr * p.grad\n",
    "  \n",
    "  # Track stats\n",
    "  if ii % 10000 == 0:\n",
    "    print(f'{ii:7d} / {max_steps:7d}: {loss.item():.4f}')\n",
    "  loss_ii.append(loss.log10().item())\n",
    "\n",
    "  # Data update ratio: how large are the changes to data relative to gradient\n",
    "  with torch.no_grad():\n",
    "    ud.append([(lr * p.grad.std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  # if ii > 1000:\n",
    "  #   break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize activation distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 4))\n",
    "legends = []\n",
    "\n",
    "for i, layer in enumerate(layers[:-1]):  # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated %.2f%%' % \n",
    "    (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density = True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
