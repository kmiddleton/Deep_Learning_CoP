{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micrograd\n",
    "\n",
    "Github library `karpathy/micrograd`\n",
    "\n",
    "- Autograd engine\n",
    "- Implements back-propagation\n",
    "- Efficiently evaluate the gradient of a loss function wrt the weights of a neural network\n",
    "- Tune the weights of a neural network efficiently\n",
    "- Builds the graph automatically\n",
    "    - Forward and backward pass\n",
    "- Backward pass: chain rule to calculate the derivatives\n",
    "    - How values effect others through the gradient\n",
    "\n",
    "Neural networks\n",
    "\n",
    "- Just mathematical expression\n",
    "- Data + weights $\\rightarrow$ Outputs\n",
    "- Backprop is not specific to neural networks\n",
    "\n",
    "Micrograd uses scalars, but torch will use tensors.\n",
    "\n",
    "- Math does not change on tensors\n",
    "- Tensors are arrays of tensors\n",
    "- Parallelized\n",
    "- Everything else is efficiency\n",
    "\n",
    "\n",
    "# Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x ** 2 - 4 * x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)\n",
    "\n",
    "xs = np.arange(-5, 5.25, 0.25)\n",
    "ys = f(xs)\n",
    "\n",
    "plt.plot(xs, ys);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives without derivatives.\n",
    "\n",
    "- NN doesn't do this symbolically.\n",
    "- What is the sensitivity of a function at a certain value of $x$?\n",
    "\n",
    "Numerical approximation of the slope at $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "x = 3.0\n",
    "\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rise: $(f(x + h) - f(x))$\n",
    "- Run: $h$\n",
    "\n",
    "Negative slopes at $x = -3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "x = -3.0\n",
    "\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero slope at $x = 2/3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "x = 2.0 / 3.0\n",
    "\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function with three scalar inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "d = a * b + c\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of $d$ as a function of $a$, $b$, and $c$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "\n",
    "# inputs\n",
    "a = 2.0\n",
    "b = -3.0\n",
    "c = 10\n",
    "\n",
    "d1 = a * b + c\n",
    "a += h\n",
    "d2 = a * b + c\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d2 - d1) / h)\n",
    "\n",
    "d1 = a * b + c\n",
    "b += h\n",
    "d2 = a * b + c\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d2 - d1) / h)\n",
    "\n",
    "d1 = a * b + c\n",
    "c += h\n",
    "d2 = a * b + c\n",
    "\n",
    "print('d1', d1)\n",
    "print('d2', d2)\n",
    "print('slope', (d2 - d1) / h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structures to hold values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Alternate mul for 2 * a\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t ** 2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "\n",
    "a = Value(2.0, label = 'a')\n",
    "b = Value(-3.0, label = 'b')\n",
    "print(a + b)\n",
    "\n",
    "c = Value(10.0, label = 'c')\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous, children, operation\n",
    "\n",
    "We now know how each value was created\n",
    "\n",
    "?? Should be -6 and 10, not 10 and -6 ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = a * b\n",
    "e.label = 'e'\n",
    "d = e + c\n",
    "d.label = 'd'\n",
    "f = Value(-2.0, label = 'f')\n",
    "L = d * f\n",
    "L.label = 'L'\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of expression graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "\n",
    "    # Build a set of nodes and edges\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # L to R\n",
    "    nodes, edges = trace(root)\n",
    "\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "\n",
    "        # for any value in the graph create a rectangular record\n",
    "        dot.node(name = uid,\n",
    "                 label = \"{ %s | data %.4f | grad %0.4f}\" % (n.label, n.data, n.grad),\n",
    "                 shape='record')\n",
    "        if n._op:\n",
    "            # if the value is a result of an operation, create an op node\n",
    "            dot.node(name = uid + n._op, label = n._op)\n",
    "\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._op, uid)\n",
    "    for n1, n2, in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of the forward pass is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation\n",
    "\n",
    "Start at $L$ and calculate the gradient in reverse.\n",
    "\n",
    "- Derivative of each node wrt $L$\n",
    "- Data is fixed, so we don't calculate the derivatives for data\n",
    "- Recursively multiply the derivatives along the path\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dL} = 1\n",
    "$$\n",
    "\n",
    "Chain rule\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "Gradient gives you the power to influence the output.\n",
    "\n",
    "\n",
    "## Back-propagation through a neuron\n",
    "\n",
    "tanh activation function\n",
    "\n",
    "- Inputs are squashed to the range -1 and 1\n",
    "- Useful in the range -2 to 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(-5, 5, 0.2), np.tanh(np.arange(-5, 5, 0.2)))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1 and x2\n",
    "x1 = Value(2.0, label = 'x1')\n",
    "x2 = Value(0.0, label = 'x2')\n",
    "\n",
    "# weights w1 and w2\n",
    "w1 = Value(-3.0, label = 'w1')\n",
    "w2 = Value(1.0, label = 'w2')\n",
    "\n",
    "# b is the bias of the neuron\n",
    "b = Value(6.8813735870195432, label = 'b')\n",
    "\n",
    "x1w1 = x1 * w1; x1w1.label = 'x1 * w1'\n",
    "x2w2 = x2 * w2; x2w2.label = 'x2 * w2'\n",
    "\n",
    "x1w1x2w1 = x1w1 + x2w2; x1w1x2w1.label = '(x1 * w1) + (x2 * w2)'\n",
    "\n",
    "# n is the cell body activation without the activation function\n",
    "n = x1w1x2w1 + b; n.label = 'n'\n",
    "\n",
    "draw_dot(n)\n",
    "\n",
    "# Implement tanh above\n",
    "o = n.tanh(); o.label = 'o'\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivatives on the weights is the important part\n",
    "\n",
    "Back-propagation manually from $o$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o.grad = 1.0\n",
    "# n.grad = 0.5\n",
    "# x1w1x2w1.grad = 0.5\n",
    "# b.grad = 0.5\n",
    "# x1w1.grad = 0.5\n",
    "# x2w2.grad = 0.5\n",
    "# x2.grad = w2.data * x2w2.grad\n",
    "# w2.grad = x2.data * x2w2.grad\n",
    "# x1.grad = w1.data * x1w1.grad\n",
    "# w1.grad = x1.data * x1w1.grad\n",
    "\n",
    "# # o = tanh(n)\n",
    "# # ? do / dn = 1 - tanh(n)**2\n",
    "\n",
    "# 1 - o.data ** 2\n",
    "\n",
    "# draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass automatically\n",
    "\n",
    "Implement `backward` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "o._backward()\n",
    "n._backward()\n",
    "b._backward()\n",
    "x1w1x2w1._backward()\n",
    "x1w1._backward()\n",
    "x2w2._backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't want to have to set `o.grad` manually. Use topological sort of the DAG left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bug: Gradient is incorrect (should be 2). Gradient is overwritten because self and other point to the same memory location.\n",
    "\n",
    "- Need to accumulate the derivatives `+=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(3.0, label = 'a')\n",
    "b = a + a; b.label = 'b'\n",
    "b.backward()\n",
    "draw_dot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With `micrograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value\n",
    "from micrograd.nn import Neuron, Layer, MLP\n",
    "\n",
    "y = [1.0, -1.0, -1.0, 1.0]\n",
    "X = [[2.0, 3.0, -1.0],\n",
    "     [3.0, -1.0, 0.5],\n",
    "     [0.5, 1.0, 1.0],\n",
    "     [1.0, 1.0, -1.0]]\n",
    "\n",
    "print(ys)\n",
    "print(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(batch_size=None):\n",
    "\n",
    "    # inline DataLoader :)\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "\n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "\n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "\n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(100):\n",
    "\n",
    "    # forward\n",
    "    total_loss, acc = loss()\n",
    "\n",
    "    # backward\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "\n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.25\n",
    "X = np.array(X)  # Convert list to NumPy array\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "\n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
