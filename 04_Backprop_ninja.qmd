---
title: "Becoming a Backprop Ninja"
subtitle: "Building makemore part 4"
format: html
---

# Setup is the same as the start of part 3

https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb

https://karpathy.ai/zero-to-hero.html

https://jacobwashburn-usda.github.io/LabProtocols/posts/DeepLearningCommunityofPractice/


```{python}
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random

%matplotlib inline
```

Read in all the names

```{python}
words = open('names.txt', 'r').read().splitlines()
print(len(words))
print(max(len(w) for w in words))
print(words[:8])
```

Build the vocabulary of characters and mappings to/from integers

```{python}
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
vocab_size = len(itos)
print(itos)
print(vocab_size)
```

Build the dataset

```{python}
block_size = 3

def build_dataset(words):
  X, Y = [], []
  for w in words:
    context = [0] * block_size
    for ch in w + '.':
      ix = stoi[ch]
      X.append(context)
      Y.append(ix)
      context = context[1:] + [ix] # crop and append

  X = torch.tensor(X)
  Y = torch.tensor(Y)
  print(X.shape, Y.shape)
  return X, Y

random.seed(42)
random.shuffle(words)
n1 = int(0.8 * len(words))
n2 = int(0.9 * len(words))

Xtr, Ytr = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xte, Yte = build_dataset(words[n2:])
```

We have trained the MLP with good loss

- Need to remove `loss.backward()` because it is ["leaky abstraction"](https://en.wikipedia.org/wiki/Leaky_abstraction)
- What if backprop doesn't ["just work"](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b?)
    -  Zero derivatives in sigmoid or ReLU activations ("dead neurons")

Function to compare manual gradients to pytorch gradients


```{python}
def cmp(s, dt, t):
  ex = torch.all(dt == t.grad).item()
  app = torch.allclose(dt, t.grad)
  maxdiff = (dt - t.grad).abs().max().item()
  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')
```

# Initialization

- Initialize to small random numbers rather than zero. Zero initialization can mask bad implementation of backward propagation

```{python}
n_embd = 10 # the dimensionality of the character embedding vectors
n_hidden = 64 # the number of neurons in the hidden layer of the MLP

g = torch.Generator().manual_seed(2147483647) # for reproducibility
C  = torch.randn((vocab_size, n_embd), generator = g)

# Layer 1
W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3)/((n_embd * block_size)**0.5)
b1 = torch.randn(n_hidden, generator = g) * 0.1 # using b1 just for fun,
                                                # it's useless because of BN

# Layer 2
W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1
b2 = torch.randn(vocab_size, generator = g) * 0.1

# BatchNorm parameters
bngain = torch.randn((1, n_hidden))*0.1 + 1.0
bnbias = torch.randn((1, n_hidden))*0.1

# Note: I am initializing many of these parameters in non-standard ways
# because sometimes initializing with e.g., all zeros could mask an incorrect
# implementation of the backward pass.

parameters = [C, W1, b1, W2, b2, bngain, bnbias]
print(sum(p.nelement() for p in parameters)) # number of parameters in total

for p in parameters:
  p.requires_grad = True
```

