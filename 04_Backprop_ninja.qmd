---
title: "Becoming a Backprop Ninja"
subtitle: "Building makemore part 4"
format: html
---

# Setup is the same as the start of part 3

https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures/makemore/makemore_part4_backprop.ipynb

https://karpathy.ai/zero-to-hero.html

https://jacobwashburn-usda.github.io/LabProtocols/posts/DeepLearningCommunityofPractice/


```{python}
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import random

%matplotlib inline
```

Read in all the names

```{python}
words = open('names.txt', 'r').read().splitlines()
print(len(words))
print(max(len(w) for w in words))
print(words[:8])
```

Build the vocabulary of characters and mappings to/from integers

```{python}
chars = sorted(list(set(''.join(words))))
stoi = {s:i+1 for i, s in enumerate(chars)}
stoi['.'] = 0
itos = {i:s for s, i in stoi.items()}
vocab_size = len(itos)
print(itos)
print(vocab_size)
```

Build the dataset

```{python}
block_size = 3

def build_dataset(words):
  X, Y = [], []
  for w in words:
    context = [0] * block_size
    for ch in w + '.':
      ix = stoi[ch]
      X.append(context)
      Y.append(ix)
      context = context[1:] + [ix] # crop and append

  X = torch.tensor(X)
  Y = torch.tensor(Y)
  print(X.shape, Y.shape)
  return X, Y

random.seed(42)
random.shuffle(words)
n1 = int(0.8 * len(words))
n2 = int(0.9 * len(words))

Xtr, Ytr = build_dataset(words[:n1])
Xdev, Ydev = build_dataset(words[n1:n2])
Xte, Yte = build_dataset(words[n2:])
```

We have trained the MLP with good loss

- Need to remove `loss.backward()` because it is ["leaky abstraction"](https://en.wikipedia.org/wiki/Leaky_abstraction)
- What if backprop doesn't ["just work"](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b?)
    -  Zero derivatives in sigmoid or ReLU activations ("dead neurons")

Function to compare manual gradients to pytorch gradients:

- Gradients that we are manually calculating
- Gradients that pytorch is calculating (assumed to be correct)

```{python}
def cmp(s, dt, t):
  ex = torch.all(dt == t.grad).item()
  app = torch.allclose(dt, t.grad)
  maxdiff = (dt - t.grad).abs().max().item()
  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')
```

# Initialization

- Initialize to small random numbers rather than zero. Zero initialization can mask bad implementation of backward propagation.
- When everything is zero, the equations simplify

```{python}
n_embd = 10   # the dimensionality of the character embedding vectors
n_hidden = 64 # the number of neurons in the hidden layer of the MLP

g = torch.Generator().manual_seed(2147483647) # for reproducibility
C  = torch.randn((vocab_size, n_embd), generator = g)

# Layer 1
W1 = torch.randn((n_embd * block_size, n_hidden), generator = g) * (5/3)/((n_embd * block_size)**0.5)
b1 = torch.randn(n_hidden, generator = g) * 0.1 # using b1 just for fun,
                                                # it's useless because of BN

# Layer 2
W2 = torch.randn((n_hidden, vocab_size), generator = g) * 0.1
b2 = torch.randn(vocab_size, generator = g) * 0.1

# BatchNorm parameters
bngain = torch.randn((1, n_hidden))*0.1 + 1.0
bnbias = torch.randn((1, n_hidden))*0.1

# Note: I am initializing many of these parameters in non-standard ways
# because sometimes initializing with e.g., all zeros could mask an incorrect
# implementation of the backward pass.

parameters = [C, W1, b1, W2, b2, bngain, bnbias]
print(sum(p.nelement() for p in parameters)) # number of parameters in total

for p in parameters:
  p.requires_grad = True
```

```{python}
batch_size = 32
n = batch_size # a shorter variable also, for convenience

# construct a minibatch
ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator = g)
Xb, Yb = Xtr[ix], Ytr[ix]  # batch X,Y
```

# Forward pass

```{python}
# Forward pass is chunked into smaller steps that can be run backward one step
# at a time

emb = C[Xb]  # embed the characters into vectors
embcat = emb.view(emb.shape[0], -1)  # concatenate the vectors

# linear layer 1
hprebn = embcat @ W1 + b1  # hidden layer preactivation

# batchnorm layer 1
bnmean1 = 1 / n * hprebn.sum(0, keepdim = True)
bndiff = hprebn - bnmean1
bndiff2 = bndiff**2
bnvar = 1 / (n - 1) * (bndiff2).sum(0, keepdim = True) # Bessel's correction dividing by n - 1
bnvar_inv = (bnvar + 1e-5)**-0.5
bnraw = bndiff * bnvar_inv
hpreact = bngain * bnvar_inv

# non-linearity
h - torch.tanh(hpreact)  # hidden layer

# linear layer 2
logits = h @ W2 + b2 # output layer

# cross entropy loss (same as F.cross_entropy(logits, Yb))
logit_maxes = logits.max(1, keepdim = True).values
norm_logits = logits - logit_maxes # Subtract max for numerical stability
counts = norm_logits.exp()
counts_sum = counts.sum(1, keepdims = True)
counts_sim_inv = counts_sum**-1  # Needed instead of 1 / counts_sum to get this to be exact
probs = counts * counts_sim_inv
logprobs = probs.log()
loss = -logprobs[range(n), Yb].mean()

# PyTorch backward pass
for p in parameters:
  p.grad = None
for t in [logprobs, probsm ]
```